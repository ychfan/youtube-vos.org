---
title: "The 3rd Large-scale Video Object Segmentation Challenge"
permalink: /challenge/2021/
excerpt: "Workshop in conjunction with CVPR 2021, virtual"
classes: wide
header:
  overlay_filter: rgba(232, 74, 39, 0.8)
  overlay_image: /assets/banner.jpg
sidebar:
  nav: "challenge"
---

## Introduction
Our workshop has three challenges for different video segmentation tasks including **semi-supervised video object segmentation**, **video instance segmentation** and **referring video object segmentation**.
The first two challenges will be very similar to previous challenges we have done with improved and augmented datasets ([details]({{ site.baseurl }}/dataset/vis/)).
The third challenge aims to segment an object referred by a given language expression in a video and requires algorithms to understand video and language jointly.


## Announcement
* Please checkout out our [workshop schedule]({{ site.baseurl }}/challenge/2021/workshop/) and [challenge leaderboard]({{ site.baseurl }}/challenge/2021/leaderboard/)


## Dates
* **May 20th**: The final competition results will be announced and top teams will be invited to give oral/poster presentations at our CVPR 2021 workshop.
* **May 5th - 14th**: Release test data and open the submission of the test results.
* **Feb 15th**: Codalab websites open for registration. Training and validation data are released.  


## Tasks
* [Track 1: Video Object Segmentation]({{ site.baseurl }}/dataset/vos/)
* [Track 2: Video Instance Segmentation]({{ site.baseurl }}/dataset/vis/)
* [Track 3: Referring Video Object Segmentation]({{ site.baseurl }}/dataset/rvos/)


## Submission
* [Track 1: Video Object Segmentation](https://competitions.codalab.org/competitions/28987)
* [Track 2: Video Instance Segmentation](https://competitions.codalab.org/competitions/28988)
* [Track 3: Referring Video Object Segmentation](https://competitions.codalab.org/competitions/29139)


## Organizers

:-:|:-:|:-:|:-:|:-:
<img src="{{ site.baseurl }}/assets/people/NingXu.jpg" width="500" alt="Ning Xu" />|<img src="{{ site.baseurl }}/assets/people/LinjieYang.jpg" width="500" alt="Linjie Yang" />|<img src="{{ site.baseurl }}/assets/people/YuchenFan.jpg" width="500" alt="Yuchen Fan" />|<img src="{{ site.baseurl }}/assets/people/YangFu.jpg" width="500" alt="Yang Fu" />|<img src="{{ site.baseurl }}/assets/people/WeiyaoLin.jpg" width="500" alt="Weiyao Lin" />
**[Ning Xu](https://sites.google.com/view/ningxu/)**<br />Adobe Research | **[Linjie Yang](https://sites.google.com/site/linjieyang89/)**<br />ByteDance AI Lab | **[Yuchen Fan](https://ychfan.github.io/)**<br />UIUC | **Yang Fu**<br />UIUC | **[Weiyao Lin](https://weiyaolin.github.io/)**<br />SJTU, China
<img src="{{ site.baseurl }}/assets/people/JianchaoYang.jpg" width="500" alt="Jianchao Yang" />|<img src="{{ site.baseurl }}/assets/people/HonghuiShi.jpg" width="500" alt="Honghui Shi" />|<img src="{{ site.baseurl }}/assets/people/JoonYoungLee.jpg" width="500" alt="Joon-Young Lee" />|<img src="{{ site.baseurl }}/assets/people/SeongukSeo.jpg" width="500" alt="Seonguk Seo" />
**Jianchao Yang**<br />ByteDance AI Lab | **[Humphrey Shi](https://www.humphreyshi.com/)**<br />UIUC | **[Joon-Young Lee](https://joonyoung-cv.github.io/)**<br />Adobe Research | **[Seonguk Seo](https://seoseong.uk/)**<br />SNU, Korea


## Contact
For dataset related questions, please feel free to contact [ytbvos@gmail.com](mailto:ytbvos@gmail.com).
For challenge related questions, you can also use codalab forums.


## Sponsors

:-:|:-:|:-:
<img src="{{ site.baseurl }}/assets/sponsors/Adobe.png" width="200" alt="Adobe" />|<img src="{{ site.baseurl }}/assets/sponsors/ByteDance.png" width="200" alt="ByteDance" />|<img src="{{ site.baseurl }}/assets/sponsors/UIUC.jpg" width="200" alt="UIUC" />
[Adobe](https://www.adobe.com/)|[ByteDance](https://www.bytedance.com/)|[UIUC](https://illinois.edu/)
